# query_llm.py
import os
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_groq import ChatGroq  # âœ… Groq LLM
from dotenv import load_dotenv
load_dotenv()
# âœ… Set Groq API Key
os.environ["GROQ_API_KEY"] = os.getenv("GROQ_API_KEY")

# âœ… Load retriever
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
db = FAISS.load_local("faiss_store", embeddings, allow_dangerous_deserialization=True)
retriever = db.as_retriever()

# âœ… Create LLM
llm = ChatGroq(model="llama3-8b-8192")

# âœ… (Optional) Prompt template
prompt_template = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are a helpful assistant answering questions from the given context.

Context:
{context}

Question:
{question}

Answer:
""",
)

# âœ… QA Chain
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    return_source_documents=False,
    chain_type_kwargs={"prompt": prompt_template}
)

# âœ… Ask user query
query = input("ðŸ§  Ask me anything: ")
result = qa.astream({"query": query})
print("\nðŸ¤– Answer:", result['result'])
